# Stress Test Report: Mercenary Application

## Executive Summary
This report details the results of stress testing the `mercenary-ai` Spring Boot application. The tests focused on the web server's throughput and capability to handle concurrent connections using the `/api/ask` endpoint.

**Status:** PASSED
**Max Throughput Observed:** ~2500 Requests Per Second (RPS)
**Max Latency (under load):** ~184ms

## Methodology
- **Tool:** Custom Python script using `urllib` and `concurrent.futures`.
- **Target:** `POST /api/ask` (simulated via `GET` with params as per controller definition).
- **Environment:** Local Development Environment (Windows).
- **Endpoint Tested:** `http://localhost:8080/api/ask?q=ping&dept=general`
  - *Note:* The "ping" query was used to isolate the application server performance from external dependencies (MongoDB/LLM Service). This measures the raw request handling capacity of the Spring Boot container.

## Test Results

| Test ID | Total Requests | Concurrency | Time (s) | RPS | Avg Latency (ms) | Success Rate |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 1 | 100 | 10 | 0.44 | 225.99 | 41.50 | 100% |
| 2 | 1,000 | 50 | 0.55 | 1824.24 | 19.00 | 100% |
| 3 | 5,000 | 200 | 1.98 | 2526.79 | 46.73 | 100% |

## PDF Ingestion Stress Test
A separate test was conducted to evaluate the system's performance when processing heavy payloads (PDF files with text and graphics).
- **Workload:** 100 Generated PDFs (variable size, text + shapes).
- **Ingestion Logic:** [SecureIngestionService](file:///d:/Projects/mercenary/src/main/java/com/jreinhal/mercenary/service/SecureIngestionService.java#20-119) (Tika extraction -> LLM Entity Extraction -> Embedding).
- **Results:**
  - **Status:** STABLE (Processing linearly)
  - **Throughput:** ~12 files/minute
  - **Avg Processing Time:** ~5.0 seconds per file
  - **Bottleneck:** The call to the external `ChatClient` (Ollama/LLM) for entity extraction is the primary scaling factor.
  - **Stability:** Application handled the sequence without crashing or memory leaks (monitored).

## Observations
1. **High Throughput:** The application successfully scaled to 200 concurrent users with no errors.
2. **Stable Latency:** Average latency remained low (<50ms) even under significant load.
3. **Resource Usage:** The application (Tomcat threads) handled the context switching efficiently.

## Deep Integration Check
A single request was sent to the full "intelligence" path (`q=What is the plan`).
- **Log Confirmation:** Application successfully logged `SENTINEL: Intel retrieved. Engaging Analysis.`
- **Performance:** This path is significantly slower (>5s) as it waits for the external AI model response.
- **Recommendation:** usage of `ChatClient` should be wrapped in `CompletableFuture` or reactive patterns if high concurrency on the AI path is expected, to avoid blocking Tomcat threads.

## Conclusion
The application container is healthy and performant for standard HTTP operations, capable of handling ~2500 RPS.
However, **Ingestion** and **RAG operations** are significantly throughput-constrained by the underlying AI model integration. 
- **Recommendation:** Implement an asynchronous job queue (e.g., using RabbitMQ or a DB-backed queue) for document ingestion to prevent client timeouts and decouple ingestion speed from user upload speed. The current synchronous approach holds the request open for ~5s, which is not scalable for mass uploads.
